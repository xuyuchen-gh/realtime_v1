# binlog怎么监听
> 1. MySQL 主从复制
     原理：这是最传统的 binlog 监听方式。主服务器上的 binlog 文件会被从服务器读取，并应用到自己的数据库中以保持与主服务器同步。
     实现：配置主服务器开启 binlog 功能，并在从服务器上设置复制参数指向主服务器。
> 2. 基于 binlog API 的工具
     原理：通过 MySQL 提供的 C API 或者其他高级语言封装的接口直接读取和解析 binlog 文件。
     工具：
     mysqlbinlog：官方提供的命令行工具，可用于转储 binlog 文件内容。它能够将 binlog 中的事件转换为 SQL 语句或者更易于处理的格式。
     Maxwell's Daemon (Maxwell)：一个开源工具，可以从 MySQL 的 binlog 中提取变更并发送 JSON 格式的变更消息到 Kafka、RabbitMQ 等消息队列。
     Canal：阿里巴巴开源的项目，模拟 MySQL 主从复制机制来捕获增量数据变更，并支持多种输出格式，如 Kafka、RocketMQ 等。
     Debezium：一个分布式平台，使用 Debezium 连接器可以监控多个数据库的 binlog，包括 MySQL，并将变更流式传输到 Kafka。
> 3. 逻辑复制
     原理：某些数据库系统提供了逻辑复制功能，允许用户订阅特定表或模式的数据变更。虽然这不是直接监听 binlog，但它是另一种捕获变更的方法。
     实现：例如 PostgreSQL 提供了逻辑解码特性，可以用来创建自定义的复制插件。
> 4.  触发器 + 用户定义函数（UDF）
      原理：在每个需要监控的表上创建触发器，在插入、更新或删除操作时调用 UDF 来记录变更信息。
      局限性：这种方法增加了数据库负载，不适合高并发场景；此外，维护成本较高。
> 5. 第三方服务
     原理：利用云服务商提供的数据库服务自带的变更数据捕获（CDC）功能，如 AWS DMS（Data Migration Service）、阿里云 DTS（Data Transmission Service）等。
     优势：无需自行管理基础设施，专注于业务逻辑开发。

# flinkCDC 底层是如何实现的
>1. Debezium
    原理：FlinkCDC 底层使用了 Debezium 来捕获数据库的变更数据。Debezium 是一个开源的分布式平台，专为 CDC 设计，支持多种数据库，包括 MySQL、PostgreSQL、MongoDB 等。
    作用：Debezium 通过解析数据库的日志（例如 MySQL 的 binlog 或 PostgreSQL 的 WAL），可以捕捉到每一行数据的变化（插入、更新、删除）。然后，这些变化会被序列化成消息格式，并发送到 Kafka 主题或其他消息队列中。
>2. Kafka Connect
    原理：Debezium 通常与 Kafka Connect 结合使用。Kafka Connect 是一个工具集，旨在简化将大量数据导入导出 Kafka 的过程。
    作用：在 FlinkCDC 的实现中，Kafka Connect 负责管理 Debezium 连接器的生命周期，并确保变更事件能够可靠地传输到 Kafka 中。这使得 Flink 可以从 Kafka 中消费这些变更事件，而无需直接连接到原始数据库。
>3. Flink Connector for Kafka
    原理：一旦变更数据被发送到了 Kafka，Flink 就可以通过其内置的 Kafka 连接器来读取消息。
    作用：Flink 的 Kafka 连接器提供了高效且容错的方式从 Kafka 主题中消费数据。它支持 checkpointing 和 exactly-once 语义，保证了数据的一致性和准确性。
>4. Flink SQL & Table API
    原理：为了简化开发人员的工作，Flink 提供了 SQL 和 Table API，允许用户通过声明式的查询语言来定义如何处理来自 Kafka 的变更数据流。
    作用：用户可以编写类似于标准 SQL 的查询，指定要处理的数据源（即 Kafka 主题）、过滤条件、聚合操作等。Flink 会自动生成相应的执行计划，并在集群中分布执行。
